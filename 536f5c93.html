<!DOCTYPE html><html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><!--replace-start-0--><!--replace-start-5--><!--replace-start-8--><title>Tree-based methods - My Zettelkasten</title><!--replace-end-8--><!--replace-end-5--><!--replace-end-0--><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Merriweather|Libre+Franklin|Roboto+Mono&amp;display=swap" rel="stylesheet" /><!--replace-start-1--><!--replace-start-4--><!--replace-start-7--><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><meta content="We don’t assume any distribution to the problem. Again, we try to find a function f that describe y = f(x). We can use it for non true linear problems." name="description" /><meta content="Tree-based methods" property="og:title" /><meta content="My Zettelkasten" property="og:site_name" /><meta content="article" property="og:type" /><script type="application/ld+json">[]</script><style type="text/css">body{background-color:#eeeeee !important;font-family:"Libre Franklin", serif !important}body .ui.container{font-family:"Libre Franklin", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Merriweather", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body .zettel-content h1#title-h1{background-color:rgba(33,133,208,0.1)}body nav.bottomPane{background-color:rgba(33,133,208,2.0e-2)}body div#footnotes{border-top-color:#2185d0}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.94999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.container.universe{padding-top:1em}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content div#footnotes ol > li > p:only-of-type{display:inline;margin-right:0.5em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f5f2f0}body div.zettel-view .zettel-content pre{overflow:auto}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.bottomPane ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body nav.bottomPane ul.context-list > li{list-style-type:lower-roman}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}body span.zettel-link-container span.zettel-link a{color:#2185d0;font-weight:bold;text-decoration:none}body span.zettel-link-container span.zettel-link a:hover{background-color:rgba(33,133,208,0.1)}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body span.zettel-link-container.errors span.zettel-link a:hover{text-decoration:none !important;cursor:not-allowed}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}</style><!-- MathJax -->
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- Prism.js -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css" rel="stylesheet" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-core.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>
<!-- Mermaid.js -->
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
    window.addEventListener("load", mermaid.initialize(
        {
            startOnLoad: true
        }
    ))
</script>
<!-- Custom CSS -->
<link href="./static/style.css" rel="stylesheet" />
<!--replace-end-7--><!--replace-end-4--><!--replace-end-1--></head><body><div class="ui fluid container universe"><!--replace-start-2--><!--replace-start-3--><!--replace-start-6--><nav class="flipped tree deemphasized" id="zettel-uptree" style="transform-origin: 50%"><ul class="root"><li><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link" title="2021-04-06T18:54"><a href="9f8d74d6.html">Methods for statistical analysis 1 (CAMAEI)</a></span></span></div><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link" title="2021-04-05T13:33"><a href="4d2b51b4.html">First semester</a></span></span></div><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link" title="2021-04-04T23:09"><a href=".">Index</a></span></span></div></li></ul></li></ul></li></ul></li></ul></nav><div class="ui text container" id="zettel-container" style="position: relative"><div class="zettel-view"><article class="ui raised attached segment zettel-content"><div class="pandoc"><h1 id="title-h1">Tree-based methods</h1><p>We don’t assume any distribution to the problem. Again, we try to find a function <span class="math inline">\(f\)</span> that describe <span class="math inline">\(y = f(x)\)</span>. We can use it for non true linear problems.</p><p>Non-parametric methods mean no big assumptions, like normally distributed, etc. These are valid alternatives when assumptions may be violated. Some examples: Wilcoxon Mann Whitney U-test, kernel-based, neural networks, etc. We will talk about <strong>decision trees</strong>.</p><h2 id="decision-trees">Decision trees</h2><p>Decision trees do not allow for weighting of the variable against each other. The decisions are binary for each step. This trees change a lot if we change the data, so they are not robust at all. This is because of the fact that is always looking for the best fit to the data, which changes so slightly the trunks decisions, which in turn ripples the effect downwards.</p><p>The idea is to partitionate the input space <span class="math inline">\(X\)</span> optimally into subspaces, and then fit a simple statistical model in each of these subspaces. We have to define was is optimal, which subspaces, and which statistical model.</p><h3 id="subspaces">Subspaces</h3><p>We partitionate the input space by one variable in each step using binary splits. In each step, we choose the variable that produces the best binary split. This is numerically feasible although one has to evaluate all <span class="math inline">\(n\)</span> values of all <span class="math inline">\(p\)</span> variables. The stepwise procedure is necessary.</p><h3 id="simple-model">Simple model</h3><p>For each subspace <span class="math inline">\(R_m\)</span>, we want to determine a value <span class="math inline">\(c_m\)</span> that is predicted for all observations in <span class="math inline">\(R_m\)</span>. We will choose <span class="math inline">\(c_m\)</span> to be the mean of <span class="math inline">\(R_m\)</span>.</p><h3 id="optimal-split">Optimal split</h3><p>We predict all observations in subspace <span class="math inline">\(R_m\)</span> with the same value <span class="math inline">\(c_m\)</span> (their mean). The optimal binary split can be determined by minimizing the well-known least squares loss function.</p><p><span class="math display">$$
\sum_{i=1}^{n}(y_i-f(x_i))^2
$$</span></p><p>where</p><p><span class="math display">$$
f(x) = \sum_{m=1}^M c_m \mathbf{1}_{\{x \in R_m \}}
$$</span></p><p>(The <span class="math inline">\(\mathbf{1}\)</span> corresponds to the indicator function of <span class="math inline">\(\{x \in R_m \}\)</span> where, if true, returns a <span class="math inline">\(1\)</span>, otherwise returns a <span class="math inline">\(0\)</span>).</p><h3 id="pruning">Pruning</h3><p>In particular for large datasets, the correct size of a tree should be discussed.</p><h2 id="classification-trees">Classification trees</h2><p>Besides regression trees, trees can also be applied to classification problems. Here, the outcome <span class="math inline">\(y\)</span> is not continuous but discrete, i.e. <span class="math inline">\(y \in \{ 1, \ldots, K \}\)</span>.</p><h3 id="simple-model-1">Simple model</h3><p>To fit regression tree, we choose the mean value of the observations that share an end node. In this case we cannot compute a mean value. If two classes appear equally often, we simply flip a coin.</p><h3 id="optimal-split-1">Optimal split</h3><p>For regression tress we choose the least square loss function. For classification trees, there are different approaches. We define <span class="math inline">\(\hat{p}_{m,k}\)</span> as the estimated probability that the subspace <span class="math inline">\(R_m\)</span> is classified to class <span class="math inline">\(k\)</span>. Two options: deviance and Gini index.</p><p><span class="math display">$$
\begin{align}
\text{Deviance} &amp;= \sum_{k=1}^K \hat{p}_{m,k}(-\log(\hat{p}_{m,k})) \\\\
\text{Gini index} &amp;= \sum_{k=1}^K \hat{p}_{m,k}(1-\hat{p}_{m,k})
\end{align}
$$</span></p><p>The best partition occurs if all observations are classified to the same class. Gini index can not be larger than <span class="math inline">\(0.5\)</span> because of how is computed. The deviance (also called cross-entropy) replaces the term <span class="math inline">\(1-\hat{p}_{m,k}\)</span> by <span class="math inline">\(-\log(\hat{p}_{m,k})\)</span>.</p><p>There are many <strong>advantages</strong> to decision trees:</p><ul><li>very easy to explain.</li><li>They are similar to human decision making.</li><li>The graphical illustration is easy to understand.</li><li>The computation of decision trees can be done very quickly.</li></ul><p>But also <strong>disadvantages</strong>:</p><ul><li>The predictive strength of decision tress is not very high. There exist better approaches.</li><li>Decision trees are not very robust. Small changes in the dataset are usually causing large changes in the tree.</li></ul><h2 id="random-forest">Random forest</h2><p>We learned that decision trees are an interesting non-parametric learning method, but their predictive strength is quite low, and also they are volatile. Random forests are techniques that solve these limitations.</p><h3 id="bootstrap">Bootstrap</h3><p>A technique that creates new datasets from a previous one. For any <span class="math inline">\(B\)</span> datasets of <span class="math inline">\(Z\)</span>, we draw an element and copy it to the new dataset. This can produce the apparition of the same data point multiple times.</p><h3 id="bagging">Bagging</h3><p>We generate <span class="math inline">\(B\)</span> datasets by bootstrapping. Fit the simple learning method on each of these datasets. Average the results from the single dataset. Intuitively one may expect that averaging does not increase the variance.</p><h3 id="definition-of-random-forest">Definition of random forest</h3><p>It is a collection of <span class="math inline">\(B\)</span> decision trees <span class="math inline">\(\{T_1, \ldots , T_B \}\)</span>. The <span class="math inline">\(B\)</span> decision trees are built on <span class="math inline">\(B\)</span> datasets that are created by bootstrapping. This may reduce the variance of the learning method. Unfortunately, such a collection of trees es not as easy to interpret as a single tree. This ois the concept of bagging applied to trees.</p><h3 id="breimans-idea">Breiman’s idea</h3><p>A further idea to reduce the variance of a decision tree.</p><p><span class="math display">$$
\text{Var} \left ( \frac{1}{B} \sum_{b=1}^B Z_b \right ) = \rho \sigma^2 + \frac{1-\rho}{B}\sigma^2
$$</span></p><p>When <span class="math inline">\(B\)</span> increases, the second term disappears; however the first term is independent of <span class="math inline">\(B\)</span>. Therefore, we would like to reduce <span class="math inline">\(\rho\)</span>, the correlation between the trees. Idea: when we build a decision tree on a bootstrap dataset, we only use <span class="math inline">\(m \leq p\)</span> random chosen candidates from the <span class="math inline">\(p\)</span> variables for each split.</p><p>How to choose <span class="math inline">\(m\)</span>? Note that classic bagging uses <span class="math inline">\(m=p\)</span>. The default for <strong>classification</strong> is <span class="math inline">\(m = \lfloor \sqrt{p} \rfloor\)</span>. The default for <strong>regression</strong> is <span class="math inline">\(m = \lfloor p/3 \rfloor\)</span>. But the parameter <span class="math inline">\(m\)</span> should be seen as tuning parameter and always be chosen according to the concrete dataset (compute the best <span class="math inline">\(m\)</span> by cross validation.</p><h3 id="overfitting">Overfitting</h3><p>One can show that random forests do not overfit the dataset when the number of trees is increased. Therefore, a random forest is only overfitting as strong as a single tree does. By pruning other modifications, overfitting of random forests can be reduced. However, this is not always done in practice because of Breiman’s trick,</p><h3 id="out-of-bag-samples">Out-of-bag samples</h3><p>Due to bootstrapping, a single tree within a random forest does usually not use all data points from the original dataset. More concretely, the following lemma holds <span class="math inline">\(\{T_1, \ldots , T_B \}\)</span> be decision trees built by bootstrapping, then in average ca. <span class="math inline">\(64\%\)</span> of the original data points are used to fit a tree <span class="math inline">\(T_b\)</span>. For each tree <span class="math inline">\(T_b\)</span>, we class the data points that were not used as <strong>out-of-bag sample</strong>.</p><p>For observation number <span class="math inline">\(i\)</span>, we can predict the outcome in all trees for which <span class="math inline">\(i\)</span> was OOB. This yields around <span class="math inline">\(B/3\)</span> predictions for observation <span class="math inline">\(i\)</span>. These predictions can be compared with the true value of observation <span class="math inline">\(i\)</span> to compute the prediction error. If we do this for all <span class="math inline">\(n\)</span> observations we get the <strong>out-of-bag error</strong>. One can use the OOB error to decide how many trees should be chosen.</p><h3 id="variable-importance">Variable importance</h3><p>The OOB samples can also be used to measure the importance of a certain variable. We can use the OOB samples of tree number <span class="math inline">\(b\)</span> to compute the predictive strength.</p></div><div class="metadata"><div class="date" title="Zettel date"><time datetime="2021-04-24T14:03">2021-04-24</time></div></div></article><nav class="ui attached segment deemphasized backlinksPane" id="neuron-backlinks-pane"><h3 class="ui header"><span title="Backlinks from folgezettel parents">Uplinks</span></h3><ul class="backlinks"><li><span class="zettel-link-container folge"><span class="zettel-link" title="2021-04-06T18:54"><a href="9f8d74d6.html">Methods for statistical analysis 1 (CAMAEI)</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><span class="zettel-link-container folge"><span class="zettel-link" title="2021-04-24T14:03"><a href="536f5c93.html">Tree-based methods</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span> — 24/04/2021</div></li></ul></li></ul></nav><nav class="ui attached segment deemphasized bottomPane" id="neuron-tags-pane"><div><span class="ui basic label zettel-tag" title="Tag">CAMAEI/lecture/07</span><span class="ui basic label zettel-tag" title="Tag">prof/Maximilian-Pilz</span><span class="ui basic label zettel-tag" title="Tag">statistics/decision-trees</span><span class="ui basic label zettel-tag" title="Tag">statistics/random-forest</span></div></nav><nav class="ui bottom attached icon compact inverted menu blue" id="neuron-nav-bar"><!--replace-start-9--><a class="item" href="." title="Home"><i class="home icon"></i></a><!--replace-end-9--><a class="right item" href="impulse.html" title="Open Impulse"><i class="wave square icon"></i></a></nav></div></div><!--replace-end-6--><!--replace-end-3--><!--replace-end-2--><div class="ui center aligned container footer-version"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron 1.9.25.1" /></a></div></div></div></body></html>
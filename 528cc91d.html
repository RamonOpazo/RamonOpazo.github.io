<!DOCTYPE html><html><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><!--replace-start-0--><!--replace-start-5--><!--replace-start-8--><title>Generalized linear models - My Zettelkasten</title><!--replace-end-8--><!--replace-end-5--><!--replace-end-0--><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><link href="https://fonts.googleapis.com/css?family=Merriweather|Libre+Franklin|Roboto+Mono&amp;display=swap" rel="stylesheet" /><!--replace-start-1--><!--replace-start-4--><!--replace-start-7--><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><meta content="Situation, we have a dataset of n patients and collected the weight and height of each patient. However, we do not want to measure the height in the future to save time. Can we predict the height from weight?" name="description" /><meta content="Generalized linear models" property="og:title" /><meta content="My Zettelkasten" property="og:site_name" /><meta content="article" property="og:type" /><script type="application/ld+json">[]</script><style type="text/css">body{background-color:#eeeeee !important;font-family:"Libre Franklin", serif !important}body .ui.container{font-family:"Libre Franklin", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Merriweather", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body .zettel-content h1#title-h1{background-color:rgba(33,133,208,0.1)}body nav.bottomPane{background-color:rgba(33,133,208,2.0e-2)}body div#footnotes{border-top-color:#2185d0}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.94999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.container.universe{padding-top:1em}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content div#footnotes ol > li > p:only-of-type{display:inline;margin-right:0.5em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f5f2f0}body div.zettel-view .zettel-content pre{overflow:auto}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.bottomPane ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body nav.bottomPane ul.context-list > li{list-style-type:lower-roman}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}body span.zettel-link-container span.zettel-link a{color:#2185d0;font-weight:bold;text-decoration:none}body span.zettel-link-container span.zettel-link a:hover{background-color:rgba(33,133,208,0.1)}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body span.zettel-link-container.errors span.zettel-link a:hover{text-decoration:none !important;cursor:not-allowed}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}</style><!-- MathJax -->
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- Prism.js -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css" rel="stylesheet" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-core.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>
<!-- Mermaid.js -->
<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
<script>
    window.addEventListener("load", mermaid.initialize(
        {
            startOnLoad: true
        }
    ))
</script>
<!-- Custom CSS -->
<link href="./static/style.css" rel="stylesheet" />
<!--replace-end-7--><!--replace-end-4--><!--replace-end-1--></head><body><div class="ui fluid container universe"><!--replace-start-2--><!--replace-start-3--><!--replace-start-6--><nav class="flipped tree deemphasized" id="zettel-uptree" style="transform-origin: 50%"><ul class="root"><li><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link" title="2021-04-06T18:54"><a href="9f8d74d6.html">Methods for statistical analysis 1 (CAMAEI)</a></span></span></div><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link" title="2021-04-05T13:33"><a href="4d2b51b4.html">First semester</a></span></span></div><ul><li><div class="forest-link"><span class="zettel-link-container"><span class="zettel-link" title="2021-04-04T23:09"><a href=".">Index</a></span></span></div></li></ul></li></ul></li></ul></li></ul></nav><div class="ui text container" id="zettel-container" style="position: relative"><div class="zettel-view"><article class="ui raised attached segment zettel-content"><div class="pandoc"><h1 id="title-h1">Generalized linear models</h1><h2 id="linear-models">Linear models</h2><p>Situation, we have a dataset of n patients and collected the weight and height of each patient. However, we do not want to measure the height in the future to save time. Can we predict the height from weight?</p><p><span class="math display">$$
height = f(weight)
$$</span></p><p>From the example, there seems to be a strong relation between weight and height. The correlation is 0.925. We would like to describe the height as a linera function:</p><p><span class="math display">$$
height = \beta_0 + \beta_1 \times weiight + \epsilon
$$</span></p><p>Which is the best linear function.</p><h3 id="least-squares-estimation">Least-squares estimation</h3><p>We want to choose <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in a sensible way. This sensible way is called the <strong>los function</strong>- A widely used loss function is the least squares loss function:</p><p><span class="math display">$$
L(f(x), y) = (f(x)-y)^2
$$</span></p><p>Where y = height, x = weight, f(x) = b1 + b2*x. We want to minimize beta 01 and 1. We can solve by derivation.</p><p>in R, linear models can be fitted with the lm() function. It returns the intercept and weight (beta 0 and 1).</p><h3 id="assumptions-on-epsilon">Assumptions on <span class="math inline">\(\epsilon\)</span></h3><p>The expectation of epsilon equals to 0, such that</p><p><span class="math display">$$
E[Y] = \beta_0 + \beta_1[x]
$$</span></p><p>We assume epsilon is normally distributed. Then we conclude that Y is also normally distributed.</p><h3 id="maximum-likelihood-estimation">Maximum likelihood estimation</h3><p>We assume a linear relationship, searching the linear function minimizes the least square loss.</p><p>There is a probabilistic way to estimate such a function.</p><p><span class="math display">$$
Y_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2)
$$</span></p><h2 id="generalized-linear-models">Generalized linear models</h2><p>For non continuous universes.</p><p>When we are only interested in predictions, it is recommended to use Non-parametric models. Nevertheless, we are also interested in the relationship between the variables (specially in clinical research). Then, we need models that allow for this interpretation and not only prediction.</p><p>So keeping the linear structure but adjusting it to the new cases is more useful.</p><h3 id="logistic-regression">Logistic regression</h3><p>Binary outcomes <span class="math inline">\(y \in \{0, 1\}\)</span>. Linear regressions are not appropriate because assume normally distributed outcomes.</p><p>Let us assume y to be binomial distributed <span class="math display">$$
y \sim \text{Binom}(1,p)
$$</span> In logistic regression, we use the link function: <span class="math display">$$
\begin{align}
g(z) &amp;= \frac{1}{1+\exp(-z)} \\\\
P[Y_i=1 \mid X=x_i] &amp;= \frac{1}{1+\exp(-x_i^T \beta)}
\end{align}
$$</span> We need gradient descent, a numerical method.</p><p>A <strong>generalized linear model</strong> consist of:</p><ul><li>the linear predictor</li><li>a probability distribution with density <span class="math inline">\(f\)</span>.</li><li>a link function g that maps <span class="math inline">\(g(X^T\beta) = E[Y \mid X]\)</span>.</li></ul><h3 id="maximum-likelihood-estimation-1">Maximum likelihood estimation</h3><p>Example, Poisson regression.</p><p>Another example is linear regression.</p><p>We cannot use any distribution, it must be in the <strong>exponential family</strong>. Uniqueness and existence of the maximum likelihood estimator are not guaranteed without further assumptions.</p><h3 id="software">Software</h3><p>generalized models are included in R’s base functions. There are easily applicable via the call of <code>glm()</code>. In Python, one can compute linear models via:</p><pre><code class="python language-python">from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression</code></pre><p><a href="https://www.kaggle.com/ronitf/heart-disease-uci">Kaggle dataset</a> used in this course in the link. The dataset contains 13 variables, which should predict heart disease (yes/no). We use a logistic regression model to do this prediction.</p><pre><code class="R language-R">log_mod &lt;- glm(heart_disease ~ ., data = A, family = &quot;Binomial&quot;, link=&quot;logit&quot;) # not correctly copied</code></pre><h3 id="conclusions">Conclusions</h3><p>Generalized linear model are a nice extension of linear models. They allow to make inference for different than just normally distributed data.</p><h2 id="loss-functions-and-linear-models">Loss functions and linear models</h2><h3 id="least-squares-loss">Least squares loss</h3><p>In a classical linear model the Leas Square Loss Function <span class="math display">$$
L(y, f(x)) = (y-f(x))^2
$$</span> is minimized.</p><p>The properties of the loss functions are:</p><ul><li>The least square loss is strictly convex.</li><li>It implies the existence and uniqueness of its minimizer.</li><li>It is continuous.</li><li>Is differentiable.</li></ul><h3 id="lipschitz-continuity">Lipschitz continuity</h3><ul><li>is not upper bounded by a constant.</li><li>is not robust.</li></ul><p>Robust statistics are statistics with a good behavior for data that are drawn from a wide distribution (and noisy). This is very important because in practice the true data distribution is unknown. A too high sensitivity to outliers is a bad property of a model. Compare the choice between mean and median for data analysis. For loss functions, Lipschitz-continuity is a valid condition to ensure robustness.</p><p>The most important properties of a loss function are:</p><ul><li>Convexity for existence and uniqueness of its minimizer.</li><li>Lipschitz-continuity for robustness.</li></ul><p>Furthermore, <strong>differentiability</strong> is a nice property.</p><h3 id="absolute-distance-loss">Absolute-distance loss</h3><p>defined as <span class="math display">$$
L(y, f(x)) = \mid y-f(x) \mid
$$</span> Properties: convex, Lipschitz continuous, not differentiable.</p><h3 id="epsilon-insensitive-loss"><span class="math inline">\(\epsilon\)</span>-insensitive loss</h3><p>Defined as <span class="math display">$$
L(y, f(x)) = \text{max} \{0, \mid y - f(x) \mid - \epsilon \}
$$</span></p><h3 id="huber-loss">Huber loss</h3><p>For <span class="math inline">\(r:= y -f(x)\)</span>, the Huber loss function is defined as:</p><p>Properties: Convex, Lipschitz-continuous, differentiable. Often used in robust statistics.</p><h3 id="classification">Classification</h3><p>We code the groups as {-1, 1}</p><h3 id="misclassification-rate">Misclassification rate</h3><p>The error when doing classification is measured by the misclassification rate.</p><p><span class="math inline">\(1\)</span> if <span class="math inline">\(yf(x) &lt; 0\)</span>, <span class="math inline">\(0\)</span> otherwise. The problem is not convex nor continuous.</p><p>The goal is to define a decision rule with low misclassification rate, there fore, we need a <strong>surrogate loss function</strong>.</p><h3 id="least-square-loss">Least square loss</h3><p><span class="math display">$$
L(y, f(x)) = (1-yf(x))^2
$$</span></p><h3 id="hinge-loss">Hinge loss</h3><p>Is convex, Lipschitz-continuous, not differentiable. It was applied for the original support-vector machines.</p><h3 id="logistic-loss">Logistic loss</h3><p>It is convex (but not strictly), Lipschitz-continuous, differentiable. It is equivalent to logistic regression.</p><p>Recommendations:</p><ul><li>Choice of the loss function depends on the problem.</li><li>The logistic loss allows estimation of <span class="math inline">\(P(Y=1 \mid X=x)\)</span>.</li><li>Hinge and logistic loss have nice theoretical properties as consistency and robustness.</li></ul><h3 id="p-values">p-values</h3><p>A further advantage of linear models is the computation of p-values. In generalized linear models, the maximum likelihood estimator is computed and p-value are obtained via a Wald test. But what to do with loss functions? For the least squares and the logistic loss, …</p><h3 id="regularization">Regularization</h3><p>In practical application, regularized models often show better results.</p></div><div class="metadata"><div class="date" title="Zettel date"><time datetime="2021-04-24T09:06">2021-04-24</time></div></div></article><nav class="ui attached segment deemphasized backlinksPane" id="neuron-backlinks-pane"><h3 class="ui header"><span title="Backlinks from folgezettel parents">Uplinks</span></h3><ul class="backlinks"><li><span class="zettel-link-container folge"><span class="zettel-link" title="2021-04-06T18:54"><a href="9f8d74d6.html">Methods for statistical analysis 1 (CAMAEI)</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc"><span class="zettel-link-container folge"><span class="zettel-link" title="2021-04-24T09:06"><a href="528cc91d.html">Generalized linear models</a><span data-nosnippet="" style="user-select: none; color: gray" title="Folgezettel">#</span></span></span> — 24/04/2021</div></li></ul></li></ul></nav><nav class="ui attached segment deemphasized bottomPane" id="neuron-tags-pane"><div><span class="ui basic label zettel-tag" title="Tag">CAMAEI/lecture/06</span><span class="ui basic label zettel-tag" title="Tag">CAMAEI/linear-models</span><span class="ui basic label zettel-tag" title="Tag">prof/Maximilian-Pilz</span></div></nav><nav class="ui bottom attached icon compact inverted menu blue" id="neuron-nav-bar"><!--replace-start-9--><a class="item" href="." title="Home"><i class="home icon"></i></a><!--replace-end-9--><a class="right item" href="impulse.html" title="Open Impulse"><i class="wave square icon"></i></a></nav></div></div><!--replace-end-6--><!--replace-end-3--><!--replace-end-2--><div class="ui center aligned container footer-version"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron 1.9.25.1" /></a></div></div></div></body></html>